{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import gym_cps\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning (Attacker & Defender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Environmnet\n",
    "env = gym.make('cps-v0')\n",
    "\n",
    "## Initialize Q-tables\n",
    "a_q_table = np.zeros((len(env.state), 4*len(env.state[0].attacker_value)))\n",
    "d_q_table = np.zeros((len(env.state), 16*len(env.state[0].defender_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "total_episodes = 100000   ## Total number of episodes\n",
    "learning_rate = 0.6       ## Degree of learning from this state; 0:nothing 1: a lot\n",
    "max_steps = 99            ## Maximum step per episodes\n",
    "gamma = 0.95              ## Discount rate; 0: 1:\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store win-factor and paths\n",
    "w_factor_human = []\n",
    "w_factor_machine = []\n",
    "\n",
    "data_path_human = []\n",
    "data_path_machine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All_paths\n",
    "paths = []\n",
    "\n",
    "## TODO: CHANGE TO w_factor_human WHEN NEEDED\n",
    "w_factor_machine = []\n",
    "w_factor_machine.append(1)\n",
    "\n",
    "# Run for life or for a specific episode amount\n",
    "for episode in range(total_episodes):\n",
    "    step = 0\n",
    "\n",
    "    if episode%10000 == 0:\n",
    "        print(\"Episode: \" + str(episode))\n",
    "    \n",
    "    ## Get Environment Parameters\n",
    "    env.observation, env.d_observation    = env.reset()\n",
    "    done                                  = env.done\n",
    "    win                                   = env.win\n",
    "    \n",
    "    \n",
    "      ## TODO: REMOVE COMMENT WHEN WE NEED TO LEARN ON SPECIFIC SUBNET\n",
    "#     observation = env.state[10]\n",
    "#     env.observation = observation\n",
    "#     env.d_observation = observation\n",
    "    \n",
    "    win_factor  = env.win_factor\n",
    "    \n",
    "    ## Set a path, and append to it\n",
    "    a_path = []\n",
    "    d_path = []\n",
    "    \n",
    "    a_path.append(env.observation.name)\n",
    "    d_path.append(env.d_observation.name)\n",
    "\n",
    "    \n",
    "    while step < max_steps:\n",
    "        step += 1\n",
    "        #####################\n",
    "        ##### Attacker action\n",
    "        #####################\n",
    "        \n",
    "        cur_observation = env.observation\n",
    "        \n",
    "        ## Exploration/Exploitation Trade-Off\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "        ## If this number is greater than epsilon: EXPLOITATION (take biggest q-value of this state.)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "\n",
    "            sorted_moves = cur_observation.moves\n",
    "            sorted_moves.sort()\n",
    "\n",
    "            action_space = np.unravel_index(np.argmax(a_q_table[cur_observation.index, :]), a_q_table.shape)\n",
    "            #(0: always zero, 1: index of the value) indexing is on 20 scale\n",
    "            \n",
    "            # Get what section [0 or 1 or 2 or 3]\n",
    "            # This is done to get the next node by using this: env.state[sorted_moves[index]]\n",
    "            index = -1\n",
    "            for i in range(4):\n",
    "                if i*5 <= action_space[1] < (i+1)*5:\n",
    "                    index = i\n",
    "                    break\n",
    "                    \n",
    "            ## Now we need to get the index of the attack_value for this specific node (next node)\n",
    "            attack_val_index = action_space[1] - (5*index)\n",
    "            \n",
    "            action = [cur_observation, env.state[sorted_moves[index]], attack_val_index]\n",
    "            \n",
    "\n",
    "        ## Else if this value is less tan or equal to epsilon: EXPLORATION (take a random attacker choice od this state.)\n",
    "        else:\n",
    "\n",
    "            action = env.action_sample_jk(cur_observation)\n",
    "\n",
    "        \n",
    "        ## Use action a and observe reward, next state.\n",
    "        new_observation, reward_a, reward_b, done, win_factor, win_step, win = env.attack_step_1_jk(action)\n",
    "        \n",
    "        ## Update Q-table\n",
    "        ## Bellman equation\n",
    "        ## Q(S,A) = Q(S,A) + lr[R(S,A) + gamma*(Q(S',A')) - Q(S,A)]\n",
    "        \n",
    "        # Get & Sort observation moves..\n",
    "        obs_moves = cur_observation.moves\n",
    "        obs_moves.sort()\n",
    "        \n",
    "        # Get section, than index value.\n",
    "        section = obs_moves.index(action[1].index)\n",
    "        indx_val = (5*section) + action[2]\n",
    "        \n",
    "        # update\n",
    "        a_q_table[cur_observation.index, indx_val] = a_q_table[cur_observation.index, indx_val] + learning_rate * (reward_a + gamma * np.max(a_q_table[new_observation.index, :] ) - a_q_table[cur_observation.index, indx_val])\n",
    "    \n",
    "        action.append(win_step)\n",
    "        \n",
    "        #####################\n",
    "        ##### Defender action\n",
    "        #####################\n",
    "\n",
    "        d_cur_observation = env.d_observation\n",
    "        \n",
    "\n",
    "        ## Exploration/Exploitation Trade-Off\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        here = False\n",
    "        ## If this number is greater than epsilon: EXPLOITATION (take biggest q-value of this state.)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            here = False\n",
    "            d_action_space = np.unravel_index(np.argmax(d_q_table[cur_observation.index, :]), d_q_table.shape)\n",
    "            #(0: always zero, 1: index of the value) indexing is on 20 scale\n",
    "        \n",
    "            # Get what section [0 or 1 or 2 or 3]\n",
    "            # This is done to get the next node by using this: env.state[sorted_moves[index]]\n",
    "            index = -1\n",
    "            for i in range(16):\n",
    "                if i*6 <= d_action_space[1] < (i+1)*6:\n",
    "                    index = i\n",
    "                    break\n",
    "                    \n",
    "            ## Now we need to get the index of the defense_value for this specific node (next node)\n",
    "            defense_val_index = d_action_space[1] - (6*index)\n",
    "        \n",
    "            d_action = [action, d_cur_observation, env.state[index], defense_val_index]\n",
    "\n",
    "        \n",
    "        ## Else if this value is less tan or equal to epsilon: EXPLORATION (take a random attacker choice od this state.)\n",
    "        else:\n",
    "            here = True\n",
    "            d_action = env.defender_action_sample_jk_jk2(action, d_cur_observation)\n",
    "            \n",
    "        ## Use action a and observe reward, next state.\n",
    "        # d_action = [action, d_observation, next_node, next_node_defense_index]\n",
    "        new_d_observation, reward_b = env.defense_step_jk_jk2(d_action)\n",
    "\n",
    "        # Get section, than index value.\n",
    "        section = d_action[2].index\n",
    "        indx_val = (6*section) + d_action[3]\n",
    "        \n",
    "     \n",
    "        # update\n",
    "        d_q_table[cur_observation.index, indx_val] = d_q_table[cur_observation.index, indx_val] + learning_rate * (reward_b)    \n",
    "        \n",
    "        ## Update Observation\n",
    "        cur_observation = new_observation\n",
    "        d_cur_observation = new_d_observation\n",
    "        \n",
    "        a_path.append(env.observation.name + \" \" + str(action[2]))\n",
    "        d_path.append(env.d_observation.name + \" \" + str(d_action[3]))\n",
    "        paths.append([a_path, d_path])\n",
    "        \n",
    "        ## Check if game is Done\n",
    "        if done:\n",
    "            ## TODO\n",
    "            w_factor_machine.append(win_factor)\n",
    "            break\n",
    "                \n",
    "    # Reduce epsilon; because we need less exploration\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_factor_machine.count(-1)) ##Attacker\n",
    "print(w_factor_machine.count(1))  ##Defender"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
